{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2adb05c1",
   "metadata": {},
   "source": [
    "# Project 4\n",
    "## Students:\n",
    " > Austin Houston,\n",
    " > Alexander Krneta\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "563a5a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "print(tf.__version__)# you may want to upgrade to 2.10.0 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddae40d9",
   "metadata": {},
   "source": [
    "\n",
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2493f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(keras.Model):\n",
    "    def __init__(self, vocab_size, embed_dim=256, num_heads=2, num_blocks=1, ff_dim=256, maxlen=80, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.num_generate = maxlen\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_blocks = num_blocks\n",
    "        self.embeddings = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.inputs = keras.Input(shape=(None, self.embed_dim))\n",
    "\n",
    "\n",
    "    def EmbeddingLayer(self):\n",
    "        # Initialize embeddings\n",
    "        self.token_embedding = layers.Embedding(input_dim=self.vocab_size, output_dim=self.embed_dim, input_length=self.num_generate)\n",
    "        self.positional_embedding = layers.Embedding(input_dim=self.num_generate, output_dim=self.embed_dim, input_length=self.num_generate, embeddings_initializer=keras.initializers.RandomUniform())\n",
    "        self.dropout = layers.Dropout(self.dropout_rate)\n",
    "\n",
    "        position_ids = tf.range(start=0, limit=tf.shape(self.inputs)[-1], delta=1, dtype=tf.int32)\n",
    "        position_embedding = self.positional_embedding(position_ids)\n",
    "        token_embedding = self.token_embedding(self.inputs)\n",
    "        self.embeddings = token_embedding + position_embedding\n",
    "\n",
    "\n",
    "    def TransformerBlock(self):\n",
    "        # Multi-Head Attention layer \n",
    "        # Sums the input to the block and the output from the first dropout\n",
    "        attention = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embed_dim)(self.embeddings, self.embeddings)\n",
    "        attention = layers.Dropout(rate=self.dropout_rate)(attention)\n",
    "        attention = layers.LayerNormalization(epsilon=1e-6)(layers.Add()([self.embeddings, attention]))\n",
    "        \n",
    "        # Feed-Forward Dense layer\n",
    "        # Sums the output of the first LayerNormalization and second dropout\n",
    "        dense = layers.Dense(units=self.ff_dim, activation='relu')(attention)\n",
    "        dense = layers.Dropout(rate=self.dropout_rate)(dense)\n",
    "        dense = layers.Dense(units=self.embed_dim)(dense)\n",
    "        dense = layers.Dropout(rate=self.dropout_rate)(dense)\n",
    "        dense = layers.LayerNormalization(epsilon=1e-6)(layers.Add()([attention, dense]))\n",
    "\n",
    "        self.outputs = layers.Dense(units=self.embed_dim)(dense)\n",
    "\n",
    "    def create_model(self,vocab_size, embed_dim, num_heads, num_blocks, ff_dim, maxlen, dropout_rate):\n",
    "        \n",
    "        self.EmbeddingLayer()\n",
    "        self.TransformerBlock()\n",
    "\n",
    "        model = tf.keras.models.Model(inputs = self.inputs, outputs=self.outputs)\n",
    "\n",
    "        # Compile the model with sparse categorical crossentropy loss and Adam optimizer\n",
    "        model.compile(\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            optimizer=keras.optimizers.Adam(),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ad747b",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7384cb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 511, 1072, 2226, 1232, 1053, 1756, 2226, 1499, 2280]\n",
      "[511, 1072, 2226, 1232, 1053, 1756, 2226, 1499, 2280, 1552]\n"
     ]
    }
   ],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, filepath):\n",
    "        # Object Attributes\n",
    "        self.text = None\n",
    "        self.vocab = None\n",
    "        self.reverse_vocab = None\n",
    "\n",
    "        # Initialize variable(s)\n",
    "        with open(filepath, 'r') as f:\n",
    "            self.text = f.read()\n",
    "\n",
    "\n",
    "    def prep_text(self):\n",
    "        self.text = self.text.lower()\n",
    "        self.text = ''.join([c for c in self.text if c.isalnum() or c.isspace()])\n",
    "    \n",
    "    def tokenize_text(self):\n",
    "        # Turn the text into a list of integers\n",
    "        self.text = self.text.split()\n",
    "        unique_words = np.unique(self.text)\n",
    "\n",
    "        # Create vocab dictionaries\n",
    "        self.vocab = {w: i+1 for i, w in enumerate(unique_words)}\n",
    "\n",
    "        # Create reverse vocab dictionary\n",
    "        self.reverse_vocab = {i+1: w for i, w in enumerate(unique_words)}\n",
    "\n",
    "        # Convert text to list of integers\n",
    "        self.text = [self.vocab[w] for w in self.text]\n",
    "  \n",
    "    def create_dataset(self):\n",
    "        self.prep_text()\n",
    "        self.tokenize_text()\n",
    "\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(0, len(self.text) - 1):\n",
    "            x.append(self.text[i])\n",
    "            y.append(self.text[i+1])\n",
    "        \n",
    "        return x, y, self.vocab, self.reverse_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af8e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset('beatles.txt')\n",
    "x, y, vocab, reverse_vocab = data.create_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3a399",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aa87d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateText:\n",
    "    def __init__(self, model, vocab):\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = keras.preprocessing.text.Tokenizer(num_words=len(vocab), char_level=True, oov_token='[UNK]')\n",
    "        self.tokenizer.fit_on_texts(self.vocab)\n",
    "\n",
    "    def generate_text(self, start_string, num_generate=100, temperature=1.0):\n",
    "        #generate text using the model and vocab, start with the start_string and generate num_generate words\n",
    "        # Convert input text to numerical sequence\n",
    "        input_sequence = self.tokenizer.texts_to_sequences([start_string])[0]\n",
    "\n",
    "        # Pad sequence to desired length\n",
    "        input_sequence = keras.preprocessing.sequence.pad_sequences([input_sequence], maxlen=num_generate, truncating='pre')\n",
    "\n",
    "        # Generate output sequence using the model\n",
    "        output_sequence = self.model.predict(input_sequence)[0]\n",
    "\n",
    "        # Apply temperature scaling to the output sequence\n",
    "        output_sequence = output_sequence / temperature\n",
    "        output_sequence = output_sequence ** 2\n",
    "        output_sequence = output_sequence / tf.reduce_sum(output_sequence)\n",
    "\n",
    "        # Sample the next token from the output distribution\n",
    "        sampled_token_index = tf.random.categorical(output_sequence, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # Convert the sampled token to its corresponding character\n",
    "        sampled_char = self.tokenizer.index_word.get(sampled_token_index, '[UNK]')\n",
    "\n",
    "        # Append the sampled character to the input text and repeat\n",
    "        output_text = start_string + sampled_char\n",
    "        while sampled_char != '[UNK]' and len(output_text) < num_generate:\n",
    "            input_sequence = keras.preprocessing.sequence.pad_sequences([input_sequence], maxlen=num_generate, truncating='pre')\n",
    "            output_sequence = self.model.predict(input_sequence)[0]\n",
    "            output_sequence = output_sequence / temperature\n",
    "            output_sequence = output_sequence ** 2\n",
    "            output_sequence = output_sequence / tf.reduce_sum(output_sequence)\n",
    "            sampled_token_index = tf.random.categorical(output_sequence, num_samples=1)[-1,0].numpy()\n",
    "            sampled_char = self.tokenizer.index_word.get(sampled_token_index, '[UNK]')\n",
    "            output_text += sampled_char\n",
    "\n",
    "        return output_text\n",
    "\n",
    "    def generate_random_text(self, num_generate=100, temperature=1.0):\n",
    "        return self.generate_text('', num_generate=num_generate, temperature=temperature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0bd9d",
   "metadata": {},
   "source": [
    "## Task 4: Model Traning and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5537ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model while periodically generating text to show progress\n",
    "def train_model(model, x ,y , batch_size=32, epochs=10):\n",
    "\n",
    "\n",
    "\n",
    "    # Train the model for each training example\n",
    "        for i in range(len(x)):\n",
    "            # Get the input and output sequences for the current example\n",
    "            input_seq = x[i:i+1]\n",
    "            target_seq = y[i:i+1]\n",
    "            \n",
    "            # Convert the input and target sequences to TensorFlow tensors\n",
    "            input_tensor = tf.convert_to_tensor(input_seq)\n",
    "            target_tensor = tf.convert_to_tensor(target_seq)\n",
    "            \n",
    "            # Generate predictions for the target sequence using the model\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(input_tensor)\n",
    "                loss = model.compiled_loss(target_tensor, predictions)\n",
    "\n",
    "            # Compute gradients and update model weights\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            \n",
    "            # Print the loss every 100 steps\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Step {i}: loss={loss.numpy():.4f}\")\n",
    "            \n",
    "            # Generate a sample output sequence every 1000 steps\n",
    "            if i % 1000 == 0:\n",
    "                output_seq = model.generate_sequence(input_seq)\n",
    "                output_text = \" \".join([reverse_vocab[tok] for tok in output_seq[0]])\n",
    "                print(f\"Sample output: {output_text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728a7595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243fa1f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "658fa81b",
   "metadata": {},
   "source": [
    "\n",
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b723a2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6855b442",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c41dc86",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3812e555",
   "metadata": {},
   "source": [
    "## How to Run Code\n",
    "\n",
    "Please include any special libraries and list your tf version here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad8329fa52fda5ea434e375b521aaa2477bf483cb1e2c0c679e9b1d37b903158"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
