{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2adb05c1",
   "metadata": {},
   "source": [
    "# Project 4\n",
    "## Students:\n",
    " > Austin Houston,\n",
    " > Alexander Krneta\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "563a5a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "print(tf.__version__)# you may want to upgrade to 2.10.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddae40d9",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c018f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs to be able to change number of heads?\n",
    "# input to Transformer block is broken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2493f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(keras.Model):\n",
    "    def __init__(self, vocab_size, embed_dim=256, num_heads=2, num_blocks=1, ff_dim=256, maxlen=80, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.max_length = maxlen\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_blocks = num_blocks\n",
    "        self.embeddings = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.inputs = keras.Input(shape=(None, self.embed_dim))\n",
    "\n",
    "\n",
    "    def EmbeddingLayer(self):\n",
    "        # Initialize embeddings\n",
    "        self.token_embedding = layers.Embedding(input_dim=self.vocab_size, output_dim=self.embed_dim, input_length=self.max_length)\n",
    "        self.positional_embedding = layers.Embedding(input_dim=self.max_length, output_dim=self.embed_dim, input_length=self.max_length, embeddings_initializer=keras.initializers.RandomUniform())\n",
    "        self.dropout = layers.Dropout(self.dropout_rate)\n",
    "\n",
    "        position_ids = tf.range(start=0, limit=tf.shape(self.inputs)[-1], delta=1, dtype=tf.int32)\n",
    "        position_embedding = self.positional_embedding(position_ids)\n",
    "        token_embedding = self.token_embedding(self.inputs)\n",
    "        self.embeddings = token_embedding + position_embedding\n",
    "\n",
    "\n",
    "    def TransformerBlock(self):\n",
    "        # Multi-Head Attention layer \n",
    "        # Sums the input to the block and the output from the first dropout\n",
    "        attention = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embed_dim)(self.embeddings, self.embeddings)\n",
    "        attention = layers.Dropout(rate=self.dropout_rate)(attention)\n",
    "        attention = layers.LayerNormalization(epsilon=1e-6)(layers.Add()([self.embeddings, attention]))\n",
    "        \n",
    "        # Feed-Forward Dense layer\n",
    "        # Sums the output of the first LayerNormalization and second dropout\n",
    "        dense = layers.Dense(units=self.ff_dim, activation='relu')(attention)\n",
    "        dense = layers.Dropout(rate=self.dropout_rate)(dense)\n",
    "        dense = layers.Dense(units=self.embed_dim)(dense)\n",
    "        dense = layers.Dropout(rate=self.dropout_rate)(dense)\n",
    "        dense = layers.LayerNormalization(epsilon=1e-6)(layers.Add()([attention, dense]))\n",
    "\n",
    "        self.outputs = layers.Dense(units=self.embed_dim)(dense)\n",
    "\n",
    "    def create_model(self,vocab_size, embed_dim, num_heads, num_blocks, ff_dim, maxlen, dropout_rate):\n",
    "        \n",
    "        self.EmbeddingLayer()\n",
    "        self.TransformerBlock()\n",
    "\n",
    "        model = tf.keras.models.Model(inputs = self.inputs, outputs=self.outputs)\n",
    "\n",
    "        # Compile the model with sparse categorical crossentropy loss and Adam optimizer\n",
    "        model.compile(\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            optimizer=keras.optimizers.Adam(),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02fa6ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 8.00 GB\n",
      "maxCacheSize: 2.67 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 16:26:16.131700: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-05-01 16:26:16.132790: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel(vocab_size = 10)\n",
    "model = model.create_model(vocab_size = 100, embed_dim=256, num_heads=2, num_blocks=1, ff_dim=256, maxlen=80, dropout_rate=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed2692be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3cefea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc561c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = model.EmbeddingLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4097bfd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None, 256, 256)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.output_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ad747b",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9a151b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs work with special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7384cb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            self.text = f.read()\n",
    "        self.vocab = None\n",
    "        self.reverse_vocab = None\n",
    "\n",
    "    def prep_text(self):\n",
    "        self.text = self.text.lower()\n",
    "        self.text = ''.join([c for c in self.text if c.isalnum() or c.isspace()])\n",
    "    \n",
    "    def tokenize_text(self):\n",
    "        words = np.unique(self.text.split())\n",
    "        self.vocab = {w: i+1 for i, w in enumerate(words)}\n",
    "        self.reverse_vocab = {i+1: w for i, w in enumerate(words)}\n",
    "        self.text = [self.vocab[w] for w in self.text.split()]\n",
    "    \n",
    "    def create_dataset(self):\n",
    "        self.prep_text()\n",
    "        self.tokenize_text()\n",
    "        x = np.array(self.text[:-1])\n",
    "        y = np.array(self.text[1:])\n",
    "        x = to_categorical(x, num_classes=len(self.vocab)+1)\n",
    "        return x, y, self.vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c1c468e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset('beatles.txt')\n",
    "x, y, vocab = data.create_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9a733d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 1,\n",
       " '1': 2,\n",
       " '1234': 3,\n",
       " '15': 4,\n",
       " '17': 5,\n",
       " '2': 6,\n",
       " '3': 7,\n",
       " '4': 8,\n",
       " '50': 9,\n",
       " '56789': 10,\n",
       " '9': 11,\n",
       " '909': 12,\n",
       " 'a': 13,\n",
       " 'aaaaahhhhhhhhhh': 14,\n",
       " 'aaaah': 15,\n",
       " 'aaahhh': 16,\n",
       " 'able': 17,\n",
       " 'about': 18,\n",
       " 'above': 19,\n",
       " 'abrigado': 20,\n",
       " 'accidents': 21,\n",
       " 'aches': 22,\n",
       " 'acorns': 23,\n",
       " 'acquainted': 24,\n",
       " 'across': 25,\n",
       " 'act': 26,\n",
       " 'acts': 27,\n",
       " 'admit': 28,\n",
       " 'advice': 29,\n",
       " 'affection': 30,\n",
       " 'afraid': 31,\n",
       " 'after': 32,\n",
       " 'afternoon': 33,\n",
       " 'again': 34,\n",
       " 'against': 35,\n",
       " 'age': 36,\n",
       " 'ago': 37,\n",
       " 'agree': 38,\n",
       " 'ah': 39,\n",
       " 'ahead': 40,\n",
       " 'aint': 41,\n",
       " 'air': 42,\n",
       " 'al': 43,\n",
       " 'albert': 44,\n",
       " 'alerted': 45,\n",
       " 'all': 46,\n",
       " 'allan': 47,\n",
       " 'allein': 48,\n",
       " 'alley': 49,\n",
       " 'almost': 50,\n",
       " 'alone': 51,\n",
       " 'along': 52,\n",
       " 'aloud': 53,\n",
       " 'already': 54,\n",
       " 'alright': 55,\n",
       " 'although': 56,\n",
       " 'always': 57,\n",
       " 'am': 58,\n",
       " 'american': 59,\n",
       " 'amore': 60,\n",
       " 'amoving': 61,\n",
       " 'amsterdam': 62,\n",
       " 'an': 63,\n",
       " 'and': 64,\n",
       " 'andarollin': 65,\n",
       " 'andern': 66,\n",
       " 'angel': 67,\n",
       " 'angry': 68,\n",
       " 'anna': 69,\n",
       " 'annoyed': 70,\n",
       " 'another': 71,\n",
       " 'answer': 72,\n",
       " 'any': 73,\n",
       " 'anybody': 74,\n",
       " 'anybodys': 75,\n",
       " 'anyhow': 76,\n",
       " 'anymore': 77,\n",
       " 'anyone': 78,\n",
       " 'anything': 79,\n",
       " 'anytime': 80,\n",
       " 'anyway': 81,\n",
       " 'anywhere': 82,\n",
       " 'apart': 83,\n",
       " 'apologize': 84,\n",
       " 'appear': 85,\n",
       " 'appears': 86,\n",
       " 'apple': 87,\n",
       " 'appointment': 88,\n",
       " 'appreciate': 89,\n",
       " 'are': 90,\n",
       " 'arent': 91,\n",
       " 'arise': 92,\n",
       " 'arizona': 93,\n",
       " 'armchair': 94,\n",
       " 'armen': 95,\n",
       " 'arms': 96,\n",
       " 'army': 97,\n",
       " 'arockin': 98,\n",
       " 'arollin': 99,\n",
       " 'arolling': 100,\n",
       " 'around': 101,\n",
       " 'arrive': 102,\n",
       " 'arrives': 103,\n",
       " 'arriving': 104,\n",
       " 'as': 105,\n",
       " 'ask': 106,\n",
       " 'asked': 107,\n",
       " 'asking': 108,\n",
       " 'asleep': 109,\n",
       " 'assure': 110,\n",
       " 'assured': 111,\n",
       " 'at': 112,\n",
       " 'ate': 113,\n",
       " 'aticking': 114,\n",
       " 'atlantic': 115,\n",
       " 'attractively': 116,\n",
       " 'attracts': 117,\n",
       " 'audience': 118,\n",
       " 'aunt': 119,\n",
       " 'avoid': 120,\n",
       " 'awail': 121,\n",
       " 'awake': 122,\n",
       " 'aware': 123,\n",
       " 'away': 124,\n",
       " 'awoke': 125,\n",
       " 'awoken': 126,\n",
       " 'ay': 127,\n",
       " 'b': 128,\n",
       " 'ba': 129,\n",
       " 'babe': 130,\n",
       " 'baby': 131,\n",
       " 'babys': 132,\n",
       " 'back': 133,\n",
       " 'backdoor': 134,\n",
       " 'backed': 135,\n",
       " 'backing': 136,\n",
       " 'bacon': 137,\n",
       " 'bad': 138,\n",
       " 'badly': 139,\n",
       " 'bag': 140,\n",
       " 'bags': 141,\n",
       " 'balalaikas': 142,\n",
       " 'ball': 143,\n",
       " 'ballad': 144,\n",
       " 'band': 145,\n",
       " 'bang': 146,\n",
       " 'banker': 147,\n",
       " 'banks': 148,\n",
       " 'barber': 149,\n",
       " 'barrow': 150,\n",
       " 'based': 151,\n",
       " 'bath': 152,\n",
       " 'bathroom': 153,\n",
       " 'bbbbb': 154,\n",
       " 'bbcbb': 155,\n",
       " 'bc': 156,\n",
       " 'be': 157,\n",
       " 'beach': 158,\n",
       " 'beam': 159,\n",
       " 'beat': 160,\n",
       " 'beatin': 161,\n",
       " 'beautiful': 162,\n",
       " 'beauty': 163,\n",
       " 'became': 164,\n",
       " 'because': 165,\n",
       " 'becomes': 166,\n",
       " 'bed': 167,\n",
       " 'bedroom': 168,\n",
       " 'beds': 169,\n",
       " 'bee': 170,\n",
       " 'been': 171,\n",
       " 'beep': 172,\n",
       " 'beepm': 173,\n",
       " 'bees': 174,\n",
       " 'beethoven': 175,\n",
       " 'before': 176,\n",
       " 'beg': 177,\n",
       " 'begged': 178,\n",
       " 'beggin': 179,\n",
       " 'begging': 180,\n",
       " 'begin': 181,\n",
       " 'begings': 182,\n",
       " 'beginning': 183,\n",
       " 'begins': 184,\n",
       " 'behave': 185,\n",
       " 'behind': 186,\n",
       " 'bei': 187,\n",
       " 'being': 188,\n",
       " 'bejust': 189,\n",
       " 'believe': 190,\n",
       " 'believing': 191,\n",
       " 'belle': 192,\n",
       " 'bells': 193,\n",
       " 'bellyful': 194,\n",
       " 'belong': 195,\n",
       " 'belonged': 196,\n",
       " 'below': 197,\n",
       " 'beman': 198,\n",
       " 'bended': 199,\n",
       " 'beneath': 200,\n",
       " 'benefit': 201,\n",
       " 'bent': 202,\n",
       " 'beside': 203,\n",
       " 'best': 204,\n",
       " 'bet': 205,\n",
       " 'better': 206,\n",
       " 'between': 207,\n",
       " 'beyond': 208,\n",
       " 'bible': 209,\n",
       " 'biding': 210,\n",
       " 'bien': 211,\n",
       " 'big': 212,\n",
       " 'bigger': 213,\n",
       " 'biggest': 214,\n",
       " 'bill': 215,\n",
       " 'billy': 216,\n",
       " 'bin': 217,\n",
       " 'bird': 218,\n",
       " 'birds': 219,\n",
       " 'birthday': 220,\n",
       " 'bishopsgate': 221,\n",
       " 'bist': 222,\n",
       " 'bit': 223,\n",
       " 'bits': 224,\n",
       " 'black': 225,\n",
       " 'blackbird': 226,\n",
       " 'blackburn': 227,\n",
       " 'blame': 228,\n",
       " 'blew': 229,\n",
       " 'blind': 230,\n",
       " 'blindly': 231,\n",
       " 'blink': 232,\n",
       " 'bloody': 233,\n",
       " 'blow': 234,\n",
       " 'blowin': 235,\n",
       " 'blows': 236,\n",
       " 'blue': 237,\n",
       " 'blueaaaaaaaahhhh': 238,\n",
       " 'blues': 239,\n",
       " 'boac': 240,\n",
       " 'board': 241,\n",
       " 'boat': 242,\n",
       " 'bone': 243,\n",
       " 'book': 244,\n",
       " 'booked': 245,\n",
       " 'books': 246,\n",
       " 'boom': 247,\n",
       " 'bootlace': 248,\n",
       " 'boots': 249,\n",
       " 'bop': 250,\n",
       " 'born': 251,\n",
       " 'both': 252,\n",
       " 'bother': 253,\n",
       " 'bottle': 254,\n",
       " 'bottom': 255,\n",
       " 'bought': 256,\n",
       " 'bound': 257,\n",
       " 'bounder': 258,\n",
       " 'bout': 259,\n",
       " 'box': 260,\n",
       " 'boy': 261,\n",
       " 'boyfriend': 262,\n",
       " 'boymother': 263,\n",
       " 'boys': 264,\n",
       " 'bra': 265,\n",
       " 'brain': 266,\n",
       " 'brand': 267,\n",
       " 'break': 268,\n",
       " 'breakfast': 269,\n",
       " 'breaking': 270,\n",
       " 'breaks': 271,\n",
       " 'breast': 272,\n",
       " 'brew': 273,\n",
       " 'bride': 274,\n",
       " 'bridge': 275,\n",
       " 'bright': 276,\n",
       " 'bring': 277,\n",
       " 'bringing': 278,\n",
       " 'brings': 279,\n",
       " 'broke': 280,\n",
       " 'broken': 281,\n",
       " 'brother': 282,\n",
       " 'brown': 283,\n",
       " 'built': 284,\n",
       " 'bulldog': 285,\n",
       " 'bulletheaded': 286,\n",
       " 'bullfrog': 287,\n",
       " 'bundle': 288,\n",
       " 'bungalow': 289,\n",
       " 'buried': 290,\n",
       " 'burning': 291,\n",
       " 'burns': 292,\n",
       " 'burst': 293,\n",
       " 'bus': 294,\n",
       " 'busby': 295,\n",
       " 'business': 296,\n",
       " 'busy': 297,\n",
       " 'but': 298,\n",
       " 'butted': 299,\n",
       " 'butterflies': 300,\n",
       " 'buy': 301,\n",
       " 'buys': 302,\n",
       " 'by': 303,\n",
       " 'bye': 304,\n",
       " 'c': 305,\n",
       " 'cake': 306,\n",
       " 'california': 307,\n",
       " 'call': 308,\n",
       " 'called': 309,\n",
       " 'calling': 310,\n",
       " 'calls': 311,\n",
       " 'came': 312,\n",
       " 'can': 313,\n",
       " 'canary': 314,\n",
       " 'canite': 315,\n",
       " 'cannot': 316,\n",
       " 'cant': 317,\n",
       " 'cap': 318,\n",
       " 'captain': 319,\n",
       " 'car': 320,\n",
       " 'carat': 321,\n",
       " 'carathon': 322,\n",
       " 'card': 323,\n",
       " 'care': 324,\n",
       " 'cares': 325,\n",
       " 'caressing': 326,\n",
       " 'carousel': 327,\n",
       " 'carry': 328,\n",
       " 'carrying': 329,\n",
       " 'carve': 330,\n",
       " 'case': 331,\n",
       " 'cast': 332,\n",
       " 'cat': 333,\n",
       " 'catch': 334,\n",
       " 'caught': 335,\n",
       " 'cause': 336,\n",
       " 'cave': 337,\n",
       " 'ceiling': 338,\n",
       " 'celebrate': 339,\n",
       " 'celebrated': 340,\n",
       " 'celebrations': 341,\n",
       " 'cellophane': 342,\n",
       " 'cent': 343,\n",
       " 'certain': 344,\n",
       " 'certainly': 345,\n",
       " 'cha': 346,\n",
       " 'chachachachancebirthday': 347,\n",
       " 'chain': 348,\n",
       " 'chains': 349,\n",
       " 'chair': 350,\n",
       " 'chairman': 351,\n",
       " 'challenge': 352,\n",
       " 'chance': 353,\n",
       " 'chances': 354,\n",
       " 'change': 355,\n",
       " 'changed': 356,\n",
       " 'changes': 357,\n",
       " 'changing': 358,\n",
       " 'charity': 359,\n",
       " 'chasing': 360,\n",
       " 'cheat': 361,\n",
       " 'check': 362,\n",
       " 'checked': 363,\n",
       " 'cherry': 364,\n",
       " 'child': 365,\n",
       " 'childlike': 366,\n",
       " 'children': 367,\n",
       " 'childrens': 368,\n",
       " 'chip': 369,\n",
       " 'chocolate': 370,\n",
       " 'choking': 371,\n",
       " 'choose': 372,\n",
       " 'chords': 373,\n",
       " 'chorus': 374,\n",
       " 'christ': 375,\n",
       " 'chuck': 376,\n",
       " 'church': 377,\n",
       " 'cia': 378,\n",
       " 'cicce': 379,\n",
       " 'cigarette': 380,\n",
       " 'city': 381,\n",
       " 'class': 382,\n",
       " 'clean': 383,\n",
       " 'clear': 384,\n",
       " 'climb': 385,\n",
       " 'climbing': 386,\n",
       " 'clinging': 387,\n",
       " 'clock': 388,\n",
       " 'close': 389,\n",
       " 'closed': 390,\n",
       " 'closer': 391,\n",
       " 'closing': 392,\n",
       " 'clothes': 393,\n",
       " 'cloud': 394,\n",
       " 'clouds': 395,\n",
       " 'cloudy': 396,\n",
       " 'clown': 397,\n",
       " 'clowns': 398,\n",
       " 'club': 399,\n",
       " 'clubs': 400,\n",
       " 'clue': 401,\n",
       " 'clutching': 402,\n",
       " 'cmon': 403,\n",
       " 'coat': 404,\n",
       " 'cocacola': 405,\n",
       " 'cocker': 406,\n",
       " 'coconut': 407,\n",
       " 'coffee': 408,\n",
       " 'cold': 409,\n",
       " 'colder': 410,\n",
       " 'collapsed': 411,\n",
       " 'college': 412,\n",
       " 'colour': 413,\n",
       " 'colourful': 414,\n",
       " 'comb': 415,\n",
       " 'come': 416,\n",
       " 'comes': 417,\n",
       " 'comfort': 418,\n",
       " 'coming': 419,\n",
       " 'command': 420,\n",
       " 'compare': 421,\n",
       " 'compares': 422,\n",
       " 'complainin': 423,\n",
       " 'comrade': 424,\n",
       " 'conceive': 425,\n",
       " 'confusing': 426,\n",
       " 'congo': 427,\n",
       " 'constitution': 428,\n",
       " 'continuing': 429,\n",
       " 'contribution': 430,\n",
       " 'controlled': 431,\n",
       " 'conversation': 432,\n",
       " 'cooking': 433,\n",
       " 'cool': 434,\n",
       " 'coral': 435,\n",
       " 'corner': 436,\n",
       " 'cornflake': 437,\n",
       " 'corporation': 438,\n",
       " 'correct': 439,\n",
       " 'cos': 440,\n",
       " 'cottage': 441,\n",
       " 'could': 442,\n",
       " 'couldnt': 443,\n",
       " 'couldt': 444,\n",
       " 'count': 445,\n",
       " 'country': 446,\n",
       " 'couple': 447,\n",
       " 'course': 448,\n",
       " 'cows': 449,\n",
       " 'crabalocker': 450,\n",
       " 'cracker': 451,\n",
       " 'cracks': 452,\n",
       " 'crash': 453,\n",
       " 'crawled': 454,\n",
       " 'crawling': 455,\n",
       " 'crazy': 456,\n",
       " 'cream': 457,\n",
       " 'creep': 458,\n",
       " 'creeps': 459,\n",
       " 'creme': 460,\n",
       " 'cried': 461,\n",
       " 'cries': 462,\n",
       " 'crime': 463,\n",
       " 'cross': 464,\n",
       " 'crossed': 465,\n",
       " 'crowd': 466,\n",
       " 'crucify': 467,\n",
       " 'cruel': 468,\n",
       " 'cry': 469,\n",
       " 'cryhihi': 470,\n",
       " 'crying': 471,\n",
       " 'cummin': 472,\n",
       " 'cup': 473,\n",
       " 'curse': 474,\n",
       " 'custard': 475,\n",
       " 'customer': 476,\n",
       " 'cut': 477,\n",
       " 'cuts': 478,\n",
       " 'd': 479,\n",
       " 'da': 480,\n",
       " 'dadadada': 481,\n",
       " 'dadandadanda': 482,\n",
       " 'daddy': 483,\n",
       " 'daddys': 484,\n",
       " 'daily': 485,\n",
       " 'daises': 486,\n",
       " 'daisy': 487,\n",
       " 'dakota': 488,\n",
       " 'damn': 489,\n",
       " 'dan': 490,\n",
       " 'dance': 491,\n",
       " 'dancebirthday': 492,\n",
       " 'danced': 493,\n",
       " 'dancer': 494,\n",
       " 'dances': 495,\n",
       " 'dancin': 496,\n",
       " 'daniel': 497,\n",
       " 'dann': 498,\n",
       " 'danny': 499,\n",
       " 'daran': 500,\n",
       " 'dark': 501,\n",
       " 'darkness': 502,\n",
       " 'darlin': 503,\n",
       " 'darling': 504,\n",
       " 'darn': 505,\n",
       " 'darning': 506,\n",
       " 'das': 507,\n",
       " 'date': 508,\n",
       " 'dave': 509,\n",
       " 'dawn': 510,\n",
       " 'day': 511,\n",
       " 'daya': 512,\n",
       " 'days': 513,\n",
       " 'de': 514,\n",
       " 'dead': 515,\n",
       " 'dear': 516,\n",
       " 'deceive': 517,\n",
       " 'decide': 518,\n",
       " 'declare': 519,\n",
       " 'deep': 520,\n",
       " 'deeper': 521,\n",
       " 'deine': 522,\n",
       " 'deinen': 523,\n",
       " 'delay': 524,\n",
       " 'deliver': 525,\n",
       " 'demonstrate': 526,\n",
       " 'den': 527,\n",
       " 'denied': 528,\n",
       " 'denis': 529,\n",
       " 'denkt': 530,\n",
       " 'denn': 531,\n",
       " 'deny': 532,\n",
       " 'department': 533,\n",
       " 'desert': 534,\n",
       " 'deserve': 535,\n",
       " 'desmond': 536,\n",
       " 'dessertyes': 537,\n",
       " 'destruction': 538,\n",
       " 'determined': 539,\n",
       " 'devil': 540,\n",
       " 'diamant': 541,\n",
       " 'diamond': 542,\n",
       " 'diamonds': 543,\n",
       " 'dich': 544,\n",
       " 'did': 545,\n",
       " 'didididi': 546,\n",
       " 'didididindi': 547,\n",
       " 'didnt': 548,\n",
       " 'die': 549,\n",
       " 'died': 550,\n",
       " 'dies': 551,\n",
       " 'difference': 552,\n",
       " 'different': 553,\n",
       " 'differnt': 554,\n",
       " 'dig': 555,\n",
       " 'digging': 556,\n",
       " 'dime': 557,\n",
       " 'dinner': 558,\n",
       " 'dir': 559,\n",
       " 'direction': 560,\n",
       " 'dirt': 561,\n",
       " 'dirty': 562,\n",
       " 'disagree': 563,\n",
       " 'disappear': 564,\n",
       " 'disappearing': 565,\n",
       " 'disappointment': 566,\n",
       " 'disconnect': 567,\n",
       " 'discovered': 568,\n",
       " 'discreetly': 569,\n",
       " 'disease': 570,\n",
       " 'diverted': 571,\n",
       " 'dizzy': 572,\n",
       " 'dj': 573,\n",
       " 'dna': 574,\n",
       " 'do': 575,\n",
       " 'doc': 576,\n",
       " 'doch': 577,\n",
       " 'dock': 578,\n",
       " 'doctor': 579,\n",
       " 'does': 580,\n",
       " 'doesnt': 581,\n",
       " 'dog': 582,\n",
       " 'doggone': 583,\n",
       " 'dogs': 584,\n",
       " 'doing': 585,\n",
       " 'donated': 586,\n",
       " 'done': 587,\n",
       " 'dont': 588,\n",
       " 'door': 589,\n",
       " 'doors': 590,\n",
       " 'doris': 591,\n",
       " 'dose': 592,\n",
       " 'doubt': 593,\n",
       " 'doubted': 594,\n",
       " 'dovetail': 595,\n",
       " 'down': 596,\n",
       " 'downstairs': 597,\n",
       " 'draems': 598,\n",
       " 'drag': 599,\n",
       " 'dragged': 600,\n",
       " 'drank': 601,\n",
       " 'dreadful': 602,\n",
       " 'dream': 603,\n",
       " 'dreaming': 604,\n",
       " 'dreams': 605,\n",
       " 'drehtest': 606,\n",
       " 'dressed': 607,\n",
       " 'dresses': 608,\n",
       " 'dressing': 609,\n",
       " 'drew': 610,\n",
       " 'drift': 611,\n",
       " 'drifting': 612,\n",
       " 'drink': 613,\n",
       " 'drinkin': 614,\n",
       " 'drinking': 615,\n",
       " 'dripping': 616,\n",
       " 'drive': 617,\n",
       " 'driver': 618,\n",
       " 'driving': 619,\n",
       " 'drop': 620,\n",
       " 'drove': 621,\n",
       " 'dry': 622,\n",
       " 'du': 623,\n",
       " 'duchess': 624,\n",
       " 'ducked': 625,\n",
       " 'dug': 626,\n",
       " 'duke': 627,\n",
       " 'duty': 628,\n",
       " 'dying': 629,\n",
       " 'dylans': 630,\n",
       " 'e': 631,\n",
       " 'each': 632,\n",
       " 'eagle': 633,\n",
       " 'ear': 634,\n",
       " 'early': 635,\n",
       " 'earn': 636,\n",
       " 'earned': 637,\n",
       " 'ears': 638,\n",
       " 'earth': 639,\n",
       " 'ease': 640,\n",
       " 'easy': 641,\n",
       " 'eat': 642,\n",
       " 'eating': 643,\n",
       " 'edgar': 644,\n",
       " 'edih': 645,\n",
       " 'edison': 646,\n",
       " 'ee': 647,\n",
       " 'eggman': 648,\n",
       " 'eggmen': 649,\n",
       " 'eht': 650,\n",
       " 'eiffel': 651,\n",
       " 'eight': 652,\n",
       " 'ein': 653,\n",
       " 'einer': 654,\n",
       " 'einmal': 655,\n",
       " 'eleanor': 656,\n",
       " 'elementary': 657,\n",
       " 'elephant': 658,\n",
       " 'elephants': 659,\n",
       " 'else': 660,\n",
       " 'em': 661,\n",
       " 'end': 662,\n",
       " 'enda': 663,\n",
       " 'endear': 664,\n",
       " 'ending': 665,\n",
       " 'endless': 666,\n",
       " 'ends': 667,\n",
       " 'engaged': 668,\n",
       " 'engine': 669,\n",
       " 'england': 670,\n",
       " 'english': 671,\n",
       " 'enjoy': 672,\n",
       " 'enjoyed': 673,\n",
       " 'enough': 674,\n",
       " 'ensemble': 675,\n",
       " 'entschuldigst': 676,\n",
       " 'equal': 677,\n",
       " 'equipped': 678,\n",
       " 'es': 679,\n",
       " 'escaping': 680,\n",
       " 'eternally': 681,\n",
       " 'even': 682,\n",
       " 'evening': 683,\n",
       " 'ever': 684,\n",
       " 'evermore': 685,\n",
       " 'every': 686,\n",
       " 'everybody': 687,\n",
       " 'everybodys': 688,\n",
       " 'everyday': 689,\n",
       " 'everyone': 690,\n",
       " 'everything': 691,\n",
       " 'everythings': 692,\n",
       " 'everywhere': 693,\n",
       " 'evolution': 694,\n",
       " 'evry': 695,\n",
       " 'evrybody': 696,\n",
       " 'evrybodys': 697,\n",
       " 'evryone': 698,\n",
       " 'evryones': 699,\n",
       " 'evrything': 700,\n",
       " 'exactly': 701,\n",
       " 'except': 702,\n",
       " 'existence': 703,\n",
       " 'expert': 704,\n",
       " 'eye': 705,\n",
       " 'eyeball': 706,\n",
       " 'eyes': 707,\n",
       " 'f': 708,\n",
       " 'face': 709,\n",
       " 'faces': 710,\n",
       " 'fade': 711,\n",
       " 'faded': 712,\n",
       " 'fair': 713,\n",
       " 'fairwhat': 714,\n",
       " 'fall': 715,\n",
       " 'falling': 716,\n",
       " 'famous': 717,\n",
       " 'fancy': 718,\n",
       " 'fanques': 719,\n",
       " 'far': 720,\n",
       " 'fare': 721,\n",
       " 'farm': 722,\n",
       " 'farther': 723,\n",
       " 'fast': 724,\n",
       " 'faster': 725,\n",
       " 'fate': 726,\n",
       " 'father': 727,\n",
       " 'fbi': 728,\n",
       " 'fears': 729,\n",
       " 'feat': 730,\n",
       " 'featuring': 731,\n",
       " 'fed': 732,\n",
       " 'feed': 733,\n",
       " 'feel': 734,\n",
       " 'feelin': 735,\n",
       " 'feeling': 736,\n",
       " 'feelings': 737,\n",
       " 'feels': 738,\n",
       " 'feet': 739,\n",
       " 'felice': 740,\n",
       " 'fell': 741,\n",
       " 'fever': 742,\n",
       " 'few': 743,\n",
       " 'fi': 744,\n",
       " 'fiddle': 745,\n",
       " 'field': 746,\n",
       " 'fields': 747,\n",
       " 'fierce': 748,\n",
       " 'fifty': 749,\n",
       " 'fight': 750,\n",
       " 'fighting': 751,\n",
       " 'fil': 752,\n",
       " 'fill': 753,\n",
       " 'filled': 754,\n",
       " 'filling': 755,\n",
       " 'film': 756,\n",
       " 'filter': 757,\n",
       " 'finally': 758,\n",
       " 'find': 759,\n",
       " 'finds': 760,\n",
       " 'fine': 761,\n",
       " 'finger': 762,\n",
       " 'fingertips': 763,\n",
       " 'fire': 764,\n",
       " 'fireman': 765,\n",
       " 'fireside': 766,\n",
       " 'first': 767,\n",
       " 'fish': 768,\n",
       " 'fishwife': 769,\n",
       " 'five': 770,\n",
       " 'fix': 771,\n",
       " 'fixing': 772,\n",
       " 'flat': 773,\n",
       " 'flattop': 774,\n",
       " 'flew': 775,\n",
       " 'flies': 776,\n",
       " 'flight': 777,\n",
       " 'flirt': 778,\n",
       " 'float': 779,\n",
       " 'floating': 780,\n",
       " 'floor': 781,\n",
       " 'flowers': 782,\n",
       " 'flowing': 783,\n",
       " 'flown': 784,\n",
       " 'flows': 785,\n",
       " 'fly': 786,\n",
       " 'flyas': 787,\n",
       " 'flys': 788,\n",
       " 'fog': 789,\n",
       " 'fold': 790,\n",
       " 'folks': 791,\n",
       " 'follow': 792,\n",
       " 'fool': 793,\n",
       " 'foolin': 794,\n",
       " 'fooling': 795,\n",
       " 'foolish': 796,\n",
       " 'foot': 797,\n",
       " 'football': 798,\n",
       " 'footsteps': 799,\n",
       " 'for': 800,\n",
       " 'fore': 801,\n",
       " 'forever': 802,\n",
       " 'forget': 803,\n",
       " 'forgive': 804,\n",
       " 'forgotten': 805,\n",
       " 'forks': 806,\n",
       " 'form': 807,\n",
       " 'found': 808,\n",
       " 'fountain': 809,\n",
       " 'four': 810,\n",
       " 'fragrant': 811,\n",
       " 'france': 812,\n",
       " 'frantic': 813,\n",
       " 'free': 814,\n",
       " 'freely': 815,\n",
       " 'freun': 816,\n",
       " 'friday': 817,\n",
       " 'friend': 818,\n",
       " 'friends': 819,\n",
       " 'frightened': 820,\n",
       " 'froh': 821,\n",
       " 'from': 822,\n",
       " 'front': 823,\n",
       " 'frown': 824,\n",
       " 'fudgereally': 825,\n",
       " 'full': 826,\n",
       " 'fun': 827,\n",
       " 'funny': 828,\n",
       " 'funtake': 829,\n",
       " 'fuse': 830,\n",
       " 'fussing': 831,\n",
       " 'future': 832,\n",
       " 'g': 833,\n",
       " 'gain': 834,\n",
       " 'gallery': 835,\n",
       " 'game': 836,\n",
       " 'games': 837,\n",
       " 'gar': 838,\n",
       " 'garden': 839,\n",
       " 'garters': 840,\n",
       " 'gas': 841,\n",
       " 'gather': 842,\n",
       " 'gave': 843,\n",
       " 'gear': 844,\n",
       " 'gee': 845,\n",
       " 'gehen': 846,\n",
       " 'gently': 847,\n",
       " 'george': 848,\n",
       " 'georgias': 849,\n",
       " 'gesehen': 850,\n",
       " 'gestern': 851,\n",
       " 'get': 852,\n",
       " 'getan': 853,\n",
       " 'gets': 854,\n",
       " 'getting': 855,\n",
       " 'ggoo': 856,\n",
       " 'gib': 857,\n",
       " 'gibraltar': 858,\n",
       " 'gideon': 859,\n",
       " 'gideons': 860,\n",
       " 'gimme': 861,\n",
       " 'gin': 862,\n",
       " 'ginger': 863,\n",
       " 'girl': 864,\n",
       " 'girlfriend': 865,\n",
       " 'girls': 866,\n",
       " 'give': 867,\n",
       " 'gives': 868,\n",
       " 'givin': 869,\n",
       " 'giving': 870,\n",
       " 'gjoob': 871,\n",
       " 'glad': 872,\n",
       " 'glass': 873,\n",
       " 'glaubst': 874,\n",
       " 'glimmering': 875,\n",
       " 'glimpse': 876,\n",
       " 'glow': 877,\n",
       " 'glucklich': 878,\n",
       " 'go': 879,\n",
       " 'goats': 880,\n",
       " 'goes': 881,\n",
       " 'gogetter': 882,\n",
       " 'goin': 883,\n",
       " 'going': 884,\n",
       " 'golden': 885,\n",
       " 'gone': 886,\n",
       " 'gonna': 887,\n",
       " 'goo': 888,\n",
       " 'good': 889,\n",
       " 'goodbye': 890,\n",
       " 'goodbyes': 891,\n",
       " 'goodlooking': 892,\n",
       " 'got': 893,\n",
       " 'gotta': 894,\n",
       " 'gown': 895,\n",
       " 'grabbed': 896,\n",
       " 'grade': 897,\n",
       " 'grandchildren': 898,\n",
       " 'grass': 899,\n",
       " 'grassmother': 900,\n",
       " 'grave': 901,\n",
       " 'greatest': 902,\n",
       " 'green': 903,\n",
       " 'greet': 904,\n",
       " 'greetings': 905,\n",
       " 'grin': 906,\n",
       " 'grinning': 907,\n",
       " 'grooving': 908,\n",
       " 'ground': 909,\n",
       " 'grow': 910,\n",
       " 'guaranteed': 911,\n",
       " 'guess': 912,\n",
       " 'guest': 913,\n",
       " 'guilty': 914,\n",
       " 'guitar': 915,\n",
       " 'gum': 916,\n",
       " 'gumboot': 917,\n",
       " 'gun': 918,\n",
       " 'guru': 919,\n",
       " 'gurus': 920,\n",
       " 'guy': 921,\n",
       " 'h': 922,\n",
       " 'hab': 923,\n",
       " 'habit': 924,\n",
       " 'had': 925,\n",
       " 'hair': 926,\n",
       " 'haired': 927,\n",
       " 'half': 928,\n",
       " 'hall': 929,\n",
       " 'hammer': 930,\n",
       " 'hand': 931,\n",
       " 'hands': 932,\n",
       " 'handy': 933,\n",
       " 'hang': 934,\n",
       " 'hanging': 935,\n",
       " 'hankerchief': 936,\n",
       " 'happen': 937,\n",
       " 'happened': 938,\n",
       " 'happens': 939,\n",
       " 'happiness': 940,\n",
       " 'happinness': 941,\n",
       " 'happy': 942,\n",
       " 'hard': 943,\n",
       " 'hardly': 944,\n",
       " 'hari': 945,\n",
       " 'harm': 946,\n",
       " 'harmony': 947,\n",
       " 'has': 948,\n",
       " 'hast': 949,\n",
       " 'hat': 950,\n",
       " 'hate': 951,\n",
       " 'hates': 952,\n",
       " 'have': 953,\n",
       " 'havent': 954,\n",
       " 'havin': 955,\n",
       " 'having': 956,\n",
       " 'hay': 957,\n",
       " 'haze': 958,\n",
       " 'he': 959,\n",
       " 'head': 960,\n",
       " 'heading': 961,\n",
       " 'heads': 962,\n",
       " 'health': 963,\n",
       " 'hear': 964,\n",
       " 'heard': 965,\n",
       " 'hearing': 966,\n",
       " 'hears': 967,\n",
       " 'heart': 968,\n",
       " 'hearted': 969,\n",
       " 'hearts': 970,\n",
       " 'heat': 971,\n",
       " 'heaven': 972,\n",
       " 'heavy': 973,\n",
       " 'heba': 974,\n",
       " 'hed': 975,\n",
       " 'hela': 976,\n",
       " 'held': 977,\n",
       " 'hell': 978,\n",
       " 'hello': 979,\n",
       " 'helloa': 980,\n",
       " 'help': 981,\n",
       " 'helping': 982,\n",
       " 'helps': 983,\n",
       " 'helter': 984,\n",
       " 'hendersons': 985,\n",
       " 'henry': 986,\n",
       " 'her': 987,\n",
       " 'here': 988,\n",
       " 'heres': 989,\n",
       " 'herself': 990,\n",
       " 'hes': 991,\n",
       " 'hewr': 992,\n",
       " 'hey': 993,\n",
       " 'hide': 994,\n",
       " 'hideaway': 995,\n",
       " 'hiding': 996,\n",
       " 'high': 997,\n",
       " 'highaaaaaaaahhhh': 998,\n",
       " 'higher': 999,\n",
       " 'highheel': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3a399",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffe1274",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateText():\n",
    "    def __init__(self, model, vocab):\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def generate_text(self, start_string, num_generate=100):\n",
    "        #generate text using the model and vocab, start with the start_string and generate num_generate words\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa87d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da44c25c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edd0bd9d",
   "metadata": {},
   "source": [
    "## Task 4: Model Traning and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b59dd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model while periodically generating text to show progress\n",
    "def train_model(model, vocab, x, y, epochs=50):\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5537ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b21ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb59a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faceae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6ddc72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "658fa81b",
   "metadata": {},
   "source": [
    "\n",
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b723a2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6855b442",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c41dc86",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3812e555",
   "metadata": {},
   "source": [
    "## How to Run Code\n",
    "\n",
    "Please include any special libraries and list your tf version here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad8329fa52fda5ea434e375b521aaa2477bf483cb1e2c0c679e9b1d37b903158"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
