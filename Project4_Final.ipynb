{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2adb05c1",
   "metadata": {},
   "source": [
    "# Project 4\n",
    "## Students:\n",
    " > Austin Houston,\n",
    " > Alexander Krneta\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "563a5a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "print(tf.__version__)# you may want to upgrade to 2.10.0 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddae40d9",
   "metadata": {},
   "source": [
    "\n",
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2493f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(keras.Model):\n",
    "    def __init__(self, vocab_size, embed_dim=256, num_heads=2, num_blocks=1, ff_dim=256, maxlen=80, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.num_generate = maxlen\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_blocks = num_blocks\n",
    "        self.embeddings = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.inputs = keras.Input(shape=(None, self.embed_dim))\n",
    "\n",
    "\n",
    "    def EmbeddingLayer(self):\n",
    "        # Initialize embeddings\n",
    "        self.token_embedding = layers.Embedding(input_dim=self.vocab_size, output_dim=self.embed_dim, input_length=self.num_generate)\n",
    "        self.positional_embedding = layers.Embedding(input_dim=self.num_generate, output_dim=self.embed_dim, input_length=self.num_generate, embeddings_initializer=keras.initializers.RandomUniform())\n",
    "        self.dropout = layers.Dropout(self.dropout_rate)\n",
    "\n",
    "        position_ids = tf.range(start=0, limit=tf.shape(self.inputs)[-1], delta=1, dtype=tf.int32)\n",
    "        position_embedding = self.positional_embedding(position_ids)\n",
    "        token_embedding = self.token_embedding(self.inputs)\n",
    "        self.embeddings = token_embedding + position_embedding\n",
    "\n",
    "\n",
    "    def TransformerBlock(self):\n",
    "        # Multi-Head Attention layer \n",
    "        # Sums the input to the block and the output from the first dropout\n",
    "        attention = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embed_dim)(self.embeddings, self.embeddings)\n",
    "        attention = layers.Dropout(rate=self.dropout_rate)(attention)\n",
    "        attention = layers.LayerNormalization(epsilon=1e-6)(layers.Add()([self.embeddings, attention]))\n",
    "        \n",
    "        # Feed-Forward Dense layer\n",
    "        # Sums the output of the first LayerNormalization and second dropout\n",
    "        dense = layers.Dense(units=self.ff_dim, activation='relu')(attention)\n",
    "        dense = layers.Dropout(rate=self.dropout_rate)(dense)\n",
    "        dense = layers.Dense(units=self.embed_dim)(dense)\n",
    "        dense = layers.Dropout(rate=self.dropout_rate)(dense)\n",
    "        dense = layers.LayerNormalization(epsilon=1e-6)(layers.Add()([attention, dense]))\n",
    "\n",
    "        self.outputs = layers.Dense(units=self.embed_dim)(dense)\n",
    "\n",
    "    def create_model(self,vocab_size, embed_dim, num_heads, num_blocks, ff_dim, maxlen, dropout_rate):\n",
    "        \n",
    "        self.EmbeddingLayer()\n",
    "        self.TransformerBlock()\n",
    "\n",
    "        model = tf.keras.models.Model(inputs = self.inputs, outputs=self.outputs)\n",
    "\n",
    "        # Compile the model with sparse categorical crossentropy loss and Adam optimizer\n",
    "        model.compile(\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            optimizer=keras.optimizers.Adam(),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ad747b",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7384cb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, filepath):\n",
    "        # Object Attributes\n",
    "        self.text = None\n",
    "        self.vocab = None\n",
    "        self.reverse_vocab = None\n",
    "\n",
    "        # Initialize variable(s)\n",
    "        with open(filepath, 'r') as f:\n",
    "            self.text = f.read()\n",
    "\n",
    "\n",
    "    def prep_text(self):\n",
    "        self.text = self.text.lower()\n",
    "        self.text = ''.join([c for c in self.text if c.isalnum() or c.isspace()])\n",
    "    \n",
    "    def tokenize_text(self):\n",
    "        # Turn the text into a list of integers\n",
    "        self.text = self.text.split()\n",
    "        unique_words = np.unique(self.text)\n",
    "\n",
    "        # Create vocab dictionaries\n",
    "        self.vocab = {w: i+1 for i, w in enumerate(unique_words)}\n",
    "\n",
    "        # Create reverse vocab dictionary\n",
    "        self.reverse_vocab = {i+1: w for i, w in enumerate(unique_words)}\n",
    "\n",
    "        # Convert text to list of integers\n",
    "        self.text = [self.vocab[w] for w in self.text]\n",
    "  \n",
    "    def create_dataset(self):\n",
    "        self.prep_text()\n",
    "        self.tokenize_text()\n",
    "\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(0, len(self.text) - 1):\n",
    "            x.append(self.text[i])\n",
    "            y.append(self.text[i+1])\n",
    "        \n",
    "        return x, y, self.vocab, self.reverse_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3a399",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aa87d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateText:\n",
    "    def __init__(self, model, vocab):\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = keras.preprocessing.text.Tokenizer(num_words=len(vocab), char_level=True, oov_token='[UNK]')\n",
    "        self.tokenizer.fit_on_texts(self.vocab)\n",
    "\n",
    "    def generate_text(self, start_string, num_generate=100, temperature=1.0):\n",
    "        #generate text using the model and vocab, start with the start_string and generate num_generate words\n",
    "        # Convert input text to numerical sequence\n",
    "        input_sequence = self.tokenizer.texts_to_sequences([start_string])[0]\n",
    "\n",
    "        # Pad sequence to desired length\n",
    "        input_sequence = keras.preprocessing.sequence.pad_sequences([input_sequence], maxlen=num_generate, truncating='pre')\n",
    "\n",
    "        # Generate output sequence using the model\n",
    "        output_sequence = self.model.predict(input_sequence)[0]\n",
    "\n",
    "        # Apply temperature scaling to the output sequence\n",
    "        output_sequence = output_sequence / temperature\n",
    "        output_sequence = output_sequence ** 2\n",
    "        output_sequence = output_sequence / tf.reduce_sum(output_sequence)\n",
    "\n",
    "        # Sample the next token from the output distribution\n",
    "        sampled_token_index = tf.random.categorical(output_sequence, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # Convert the sampled token to its corresponding character\n",
    "        sampled_char = self.tokenizer.index_word.get(sampled_token_index, '[UNK]')\n",
    "\n",
    "        # Append the sampled character to the input text and repeat\n",
    "        output_text = start_string + sampled_char\n",
    "        while sampled_char != '[UNK]' and len(output_text) < num_generate:\n",
    "            input_sequence = keras.preprocessing.sequence.pad_sequences([input_sequence], maxlen=num_generate, truncating='pre')\n",
    "            output_sequence = self.model.predict(input_sequence)[0]\n",
    "            output_sequence = output_sequence / temperature\n",
    "            output_sequence = output_sequence ** 2\n",
    "            output_sequence = output_sequence / tf.reduce_sum(output_sequence)\n",
    "            sampled_token_index = tf.random.categorical(output_sequence, num_samples=1)[-1,0].numpy()\n",
    "            sampled_char = self.tokenizer.index_word.get(sampled_token_index, '[UNK]')\n",
    "            output_text += sampled_char\n",
    "\n",
    "        return output_text\n",
    "\n",
    "    def generate_random_text(self, num_generate=100, temperature=1.0):\n",
    "        return self.generate_text('', num_generate=num_generate, temperature=temperature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0bd9d",
   "metadata": {},
   "source": [
    "## Task 4: Model Traning and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "728a7595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model while periodically generating text to show progress\n",
    "def train_model(model, vocab, x, y, epochs = 10):\n",
    "    \n",
    "    # Create a dictionary to map the token IDs to words in the vocabulary\n",
    "    reverse_vocab = dict((i, word) for word, i in vocab.items())\n",
    "\n",
    "    # Train the model for the specified number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        # Shuffle the training data for each epoch\n",
    "        \n",
    "        # Train the model for each training example\n",
    "        for i in range(len(x)):\n",
    "            # Get the input and output sequences for the current example\n",
    "            input_seq = x[i:i+1]\n",
    "            target_seq = y[i:i+1]\n",
    "            \n",
    "            # Convert the input and target sequences to TensorFlow tensors\n",
    "            input_tensor = tf.convert_to_tensor(input_seq)\n",
    "            target_tensor = tf.convert_to_tensor(target_seq)\n",
    "            \n",
    "            # Generate predictions for the target sequence using the model\n",
    "            with tf.GradientTape() as tape:\n",
    "                predictions = model(input_tensor)\n",
    "                loss = model.compiled_loss(target_tensor, predictions)\n",
    "\n",
    "            # Compute gradients and update model weights\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            model.optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            \n",
    "            # Print the loss every 100 steps\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Step {i}: loss={loss.numpy():.4f}\")\n",
    "            \n",
    "            # Generate a sample output sequence every 1000 steps\n",
    "            if i % 1000 == 0:\n",
    "                output_seq = model.generate_sequence(input_seq)\n",
    "                output_text = \" \".join([reverse_vocab[tok] for tok in output_seq[0]])\n",
    "                print(f\"Sample output: {output_text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "412eb56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)           [(None, None, 256)]  0           []                               \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_3 (TFOpLamb  (3,)                0           ['input_5[0][0]']                \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_3 (Sl  ()                  0           ['tf.compat.v1.shape_3[0][0]']   \n",
      " icingOpLambda)                                                                                   \n",
      "                                                                                                  \n",
      " tf.range_3 (TFOpLambda)        (256,)               0           ['tf.__operators__.getitem_3[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " embedding_6 (Embedding)        (None, None, 256, 2  25600       ['input_5[0][0]']                \n",
      "                                56)                                                               \n",
      "                                                                                                  \n",
      " embedding_7 (Embedding)        (256, 256)           20480       ['tf.range_3[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, None, 256, 2  0          ['embedding_6[0][0]',            \n",
      " mbda)                          56)                               'embedding_7[0][0]']            \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, None, 256, 2  526080     ['tf.__operators__.add_3[0][0]', \n",
      " eadAttention)                  56)                               'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, None, 256, 2  0           ['multi_head_attention_3[0][0]'] \n",
      "                                56)                                                               \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, None, 256, 2  0           ['tf.__operators__.add_3[0][0]', \n",
      "                                56)                               'dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, None, 256, 2  512        ['add_6[0][0]']                  \n",
      " rmalization)                   56)                                                               \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, None, 256, 2  65792       ['layer_normalization_6[0][0]']  \n",
      "                                56)                                                               \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, None, 256, 2  0           ['dense_9[0][0]']                \n",
      "                                56)                                                               \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, None, 256, 2  65792       ['dropout_14[0][0]']             \n",
      "                                56)                                                               \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, None, 256, 2  0           ['dense_10[0][0]']               \n",
      "                                56)                                                               \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, None, 256, 2  0           ['layer_normalization_6[0][0]',  \n",
      "                                56)                               'dropout_15[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, None, 256, 2  512        ['add_7[0][0]']                  \n",
      " rmalization)                   56)                                                               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, None, 256, 2  65792       ['layer_normalization_7[0][0]']  \n",
      "                                56)                                                               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 770,560\n",
      "Trainable params: 770,560\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = TransformerModel(vocab_size = 100)\n",
    "model = model.create_model(vocab_size = 100, embed_dim=256, num_heads=2, num_blocks=1, ff_dim=256, maxlen=80, dropout_rate=0.1)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "243fa1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 22:18:04.827990: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at einsum_op_impl.h:502 : INVALID_ARGUMENT: Expected input 0 to have rank 4 but got: 2\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer \"query\" (type EinsumDense).\n\nExpected input 0 to have rank 4 but got: 2 [Op:Einsum]\n\nCall arguments received by layer \"query\" (type EinsumDense):\n  • inputs=tf.Tensor(shape=(1, 256), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m Dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbeatles.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m x, y, vocab, reverse_vocab \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcreate_dataset()\n\u001b[0;32m----> 6\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 24\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, vocab, x, y, epochs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Generate predictions for the target sequence using the model\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 24\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompiled_loss(target_tensor, predictions)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Compute gradients and update model weights\u001b[39;00m\n",
      "File \u001b[0;32m~/tensorflow_env/env/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/tensorflow_env/env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:7164\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7163\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 7164\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"query\" (type EinsumDense).\n\nExpected input 0 to have rank 4 but got: 2 [Op:Einsum]\n\nCall arguments received by layer \"query\" (type EinsumDense):\n  • inputs=tf.Tensor(shape=(1, 256), dtype=float32)"
     ]
    }
   ],
   "source": [
    "model = TransformerModel(vocab_size = len(np.unique(vocab)))\n",
    "model = model.create_model(vocab_size = len(np.unique(vocab)), embed_dim=256, num_heads=2, num_blocks=1, ff_dim=256, maxlen=80, dropout_rate=0.1)\n",
    "data = Dataset('beatles.txt')\n",
    "x, y, vocab, reverse_vocab = data.create_dataset()\n",
    "\n",
    "train_model(model, vocab, x, y, epochs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658fa81b",
   "metadata": {},
   "source": [
    "\n",
    "# Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c3bc8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7b723a2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6855b442",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c41dc86",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3812e555",
   "metadata": {},
   "source": [
    "## How to Run Code\n",
    "\n",
    "Please include any special libraries and list your tf version here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad8329fa52fda5ea434e375b521aaa2477bf483cb1e2c0c679e9b1d37b903158"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
