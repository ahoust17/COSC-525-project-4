{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2adb05c1",
   "metadata": {},
   "source": [
    "# Project 4\n",
    "## Students:\n",
    " > Austin Houston,\n",
    " > Alexander Krneta\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "563a5a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "print(tf.__version__)# you may want to upgrade to 2.10.0 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ddae40d9",
   "metadata": {},
   "source": [
    "\n",
    "## Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2493f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(keras.Model):\n",
    "    def __init__(self, vocab_size, embed_dim=256, num_heads=2, num_blocks=1, ff_dim=256, maxlen=80, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.maxlen = maxlen\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_blocks = num_blocks\n",
    "        self.embeddings = None\n",
    "        self.outputs = None\n",
    "\n",
    "        self.inputs = keras.Input(shape=(self.maxlen, self.embed_dim))\n",
    "\n",
    "\n",
    "    def EmbeddingLayer(self):\n",
    "        # Initialize embeddings\n",
    "        self.token_embedding = layers.Embedding(input_dim=self.vocab_size, output_dim=self.embed_dim, input_length=self.maxlen)\n",
    "        self.positional_embedding = layers.Embedding(input_dim=self.maxlen, output_dim=self.embed_dim, input_length=self.maxlen, embeddings_initializer=keras.initializers.RandomUniform())\n",
    "        self.dropout = layers.Dropout(self.dropout_rate)\n",
    "\n",
    "        position_ids = tf.range(start=0, limit=tf.shape(self.inputs)[-1], delta=1, dtype=tf.int32)\n",
    "        position_embedding = self.positional_embedding(position_ids)\n",
    "        token_embedding = self.token_embedding(self.inputs)\n",
    "        self.embeddings = token_embedding + position_embedding\n",
    "\n",
    "\n",
    "    def TransformerBlock(self):\n",
    "        # Multi-Head Attention layer \n",
    "        # Sums the input to the block and the output from the first dropout\n",
    "        attention = layers.MultiHeadAttention(num_heads=self.num_heads, key_dim=self.embed_dim)(self.embeddings, self.embeddings)\n",
    "        attention = layers.Dropout(rate=self.dropout_rate)(attention)\n",
    "        attention = layers.LayerNormalization(epsilon=1e-6)(layers.Add()([self.embeddings, attention]))\n",
    "        \n",
    "        # Feed-Forward Dense layer\n",
    "        # Sums the output of the first LayerNormalization and second dropout\n",
    "        dense = layers.Dense(units=self.ff_dim, activation='relu')(attention)\n",
    "        dense = layers.Dropout(rate=self.dropout_rate)(dense)\n",
    "        dense = layers.Dense(units=self.embed_dim)(dense)\n",
    "        dense = layers.Dropout(rate=self.dropout_rate)(dense)\n",
    "        dense = layers.LayerNormalization(epsilon=1e-6)(layers.Add()([attention, dense]))\n",
    "\n",
    "        self.outputs = layers.Dense(units=self.embed_dim)(dense)\n",
    "\n",
    "    def create_model(self,vocab_size, embed_dim, num_heads, num_blocks, ff_dim, maxlen, dropout_rate):\n",
    "        \n",
    "        self.EmbeddingLayer()\n",
    "        self.TransformerBlock()\n",
    "\n",
    "        model = tf.keras.models.Model(inputs = self.inputs, outputs=self.outputs)\n",
    "\n",
    "        # Compile the model with sparse categorical crossentropy loss and Adam optimizer\n",
    "        model.compile(\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            optimizer=keras.optimizers.Adam(),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ad747b",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7384cb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, filepath):\n",
    "        # Object Attributes\n",
    "        self.text = None\n",
    "        self.vocab = None\n",
    "        self.reverse_vocab = None\n",
    "\n",
    "        # Initialize variable(s)\n",
    "        with open(filepath, 'r') as f:\n",
    "            self.text = f.read()\n",
    "\n",
    "\n",
    "    def prep_text(self):\n",
    "        self.text = self.text.lower()\n",
    "        self.text = ''.join([c for c in self.text if c.isalnum() or c.isspace()])\n",
    "    \n",
    "    def tokenize_text(self):\n",
    "        # Turn the text into a list of integers\n",
    "        self.text = self.text.split()\n",
    "        unique_words = np.unique(self.text)\n",
    "\n",
    "        # Create vocab dictionaries\n",
    "        self.vocab = {w: i+1 for i, w in enumerate(unique_words)}\n",
    "\n",
    "        # Create reverse vocab dictionary\n",
    "        self.reverse_vocab = {i+1: w for i, w in enumerate(unique_words)}\n",
    "\n",
    "        # Convert text to list of integers\n",
    "        self.text = [self.vocab[w] for w in self.text]\n",
    "  \n",
    "    def create_dataset(self):\n",
    "        self.prep_text()\n",
    "        self.tokenize_text()\n",
    "\n",
    "        x = []\n",
    "        y = []\n",
    "        for i in range(0, len(self.text) - 1):\n",
    "            x.append(self.text[i])\n",
    "            y.append(self.text[i+1])\n",
    "        \n",
    "        return x, y, self.vocab, self.reverse_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3a399",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aa87d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateText:\n",
    "    def __init__(self, model, vocab):\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = keras.preprocessing.text.Tokenizer(num_words=len(vocab), char_level=True, oov_token='[UNK]')\n",
    "        self.tokenizer.fit_on_texts(self.vocab)\n",
    "\n",
    "    def generate_text(self, start_string, num_generate=100, temperature=1.0):\n",
    "        #generate text using the model and vocab, start with the start_string and generate num_generate words\n",
    "        # Convert input text to numerical sequence\n",
    "        input_sequence = self.tokenizer.texts_to_sequences([start_string])[0]\n",
    "\n",
    "        # Pad sequence to desired length\n",
    "        input_sequence = keras.preprocessing.sequence.pad_sequences([input_sequence], maxlen=num_generate, truncating='pre')\n",
    "\n",
    "        # Generate output sequence using the model\n",
    "        output_sequence = self.model.predict(input_sequence)[0]\n",
    "\n",
    "        # Apply temperature scaling to the output sequence\n",
    "        output_sequence = output_sequence / temperature\n",
    "        output_sequence = output_sequence ** 2\n",
    "        output_sequence = output_sequence / tf.reduce_sum(output_sequence)\n",
    "\n",
    "        # Sample the next token from the output distribution\n",
    "        sampled_token_index = tf.random.categorical(output_sequence, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # Convert the sampled token to its corresponding character\n",
    "        sampled_char = self.tokenizer.index_word.get(sampled_token_index, '[UNK]')\n",
    "\n",
    "        # Append the sampled character to the input text and repeat\n",
    "        output_text = start_string + sampled_char\n",
    "        while sampled_char != '[UNK]' and len(output_text) < num_generate:\n",
    "            input_sequence = keras.preprocessing.sequence.pad_sequences([input_sequence], maxlen=num_generate, truncating='pre')\n",
    "            output_sequence = self.model.predict(input_sequence)[0]\n",
    "            output_sequence = output_sequence / temperature\n",
    "            output_sequence = output_sequence ** 2\n",
    "            output_sequence = output_sequence / tf.reduce_sum(output_sequence)\n",
    "            sampled_token_index = tf.random.categorical(output_sequence, num_samples=1)[-1,0].numpy()\n",
    "            sampled_char = self.tokenizer.index_word.get(sampled_token_index, '[UNK]')\n",
    "            output_text += sampled_char\n",
    "\n",
    "        return output_text\n",
    "\n",
    "    def generate_random_text(self, num_generate=100, temperature=1.0):\n",
    "        return self.generate_text('', num_generate=num_generate, temperature=temperature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0bd9d",
   "metadata": {},
   "source": [
    "## Task 4: Model Traning and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "728a7595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model while periodically generating text to show progress\n",
    "def train_model(model, x, y, vocab, reverse_vocab, epochs = 10):\n",
    "\n",
    "    # for epoch in epochs\n",
    "\n",
    "        # get the sequences for each training instance\n",
    "\n",
    "        # predict on the sequence and calculate loss\n",
    "\n",
    "        # update the model\n",
    "\n",
    "        # generate text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "412eb56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 80, 256)]    0           []                               \n",
      "                                                                                                  \n",
      " tf.compat.v1.shape_10 (TFOpLam  (3,)                0           ['input_13[0][0]']               \n",
      " bda)                                                                                             \n",
      "                                                                                                  \n",
      " tf.__operators__.getitem_10 (S  ()                  0           ['tf.compat.v1.shape_10[0][0]']  \n",
      " licingOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.range_10 (TFOpLambda)       (256,)               0           ['tf.__operators__.getitem_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " embedding_20 (Embedding)       (None, 80, 256, 256  664064      ['input_13[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " embedding_21 (Embedding)       (256, 256)           20480       ['tf.range_10[0][0]']            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_10 (TFOpL  (None, 80, 256, 256  0          ['embedding_20[0][0]',           \n",
      " ambda)                         )                                 'embedding_21[0][0]']           \n",
      "                                                                                                  \n",
      " multi_head_attention_10 (Multi  (None, 80, 256, 256  526080     ['tf.__operators__.add_10[0][0]',\n",
      " HeadAttention)                 )                                 'tf.__operators__.add_10[0][0]']\n",
      "                                                                                                  \n",
      " dropout_41 (Dropout)           (None, 80, 256, 256  0           ['multi_head_attention_10[0][0]']\n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_20 (Add)                   (None, 80, 256, 256  0           ['tf.__operators__.add_10[0][0]',\n",
      "                                )                                 'dropout_41[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_20 (LayerN  (None, 80, 256, 256  512        ['add_20[0][0]']                 \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 80, 256, 256  65792       ['layer_normalization_20[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_42 (Dropout)           (None, 80, 256, 256  0           ['dense_30[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 80, 256, 256  65792       ['dropout_42[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_43 (Dropout)           (None, 80, 256, 256  0           ['dense_31[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " add_21 (Add)                   (None, 80, 256, 256  0           ['layer_normalization_20[0][0]', \n",
      "                                )                                 'dropout_43[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_21 (LayerN  (None, 80, 256, 256  512        ['add_21[0][0]']                 \n",
      " ormalization)                  )                                                                 \n",
      "                                                                                                  \n",
      " dense_32 (Dense)               (None, 80, 256, 256  65792       ['layer_normalization_21[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,409,024\n",
      "Trainable params: 1,409,024\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-06 22:42:28.701471: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at einsum_op_impl.h:502 : INVALID_ARGUMENT: Expected input 0 to have rank 4 but got: 2\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer \"query\" (type EinsumDense).\n\nExpected input 0 to have rank 4 but got: 2 [Op:Einsum]\n\nCall arguments received by layer \"query\" (type EinsumDense):\n  • inputs=tf.Tensor(shape=(1, 256), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcreate_model(vocab_size \u001b[38;5;241m=\u001b[39m vocab_size, embed_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_blocks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, ff_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m80\u001b[39m, dropout_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39msummary())\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 24\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, vocab, x, y, epochs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Generate predictions for the target sequence using the model\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m---> 24\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompiled_loss(target_tensor, predictions)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Compute gradients and update model weights\u001b[39;00m\n",
      "File \u001b[0;32m~/tensorflow_env/env/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/tensorflow_env/env/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:7164\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7163\u001b[0m   e\u001b[39m.\u001b[39mmessage \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39m name: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m name \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 7164\u001b[0m   \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"query\" (type EinsumDense).\n\nExpected input 0 to have rank 4 but got: 2 [Op:Einsum]\n\nCall arguments received by layer \"query\" (type EinsumDense):\n  • inputs=tf.Tensor(shape=(1, 256), dtype=float32)"
     ]
    }
   ],
   "source": [
    "data = Dataset('beatles.txt')\n",
    "x, y, vocab, reverse_vocab = data.create_dataset()\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = TransformerModel(vocab_size = vocab_size)\n",
    "model = model.create_model(vocab_size = vocab_size, embed_dim=256, num_heads=2, num_blocks=1, ff_dim=256, maxlen=80, dropout_rate=0.1)\n",
    "print(model.summary())\n",
    "\n",
    "train_model(model, vocab, x, y, epochs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658fa81b",
   "metadata": {},
   "source": [
    "\n",
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c3bc8c",
   "metadata": {},
   "source": [
    "Our goal with this project is to create a transformer neural network that generates Beatles songs.\n",
    "We are to use keras and tensorflow to design the network, and train it on the lyrics of 246 Beatles songs which have been tokenized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b723a2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27cbe3e1",
   "metadata": {},
   "source": [
    "This network is a transformer-based model for text generation.  The model uses a multi-head attention, enabling the model to 'attend' to different aspects of the input data, such as positioin in the sentence and position in the high-dimmensional vocabulary map (these are the only attentions utilized here).  To enable this behavior, the input data is embedded as the first layer of the model, and these embeddings are learned.  In theory, this combination can teach the model which keywords are important in which part of the phrase.  The model then feed into dense layers with normalizations and skip layers as shown in the assignment diagram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6855b442",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2ddc2adf",
   "metadata": {},
   "source": [
    "To start, our network was not able to generate results from any models. So we will not be able to compare and contrast the results.\n",
    "Here, I would hypothesize what would have happened. The model which would have been trained on one epoch would have produced nonsense, in all practicallity, it would be randomly spitting out garbage. Then the over trained model would not have any creative freedom, and would output the songs verbatim."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c41dc86",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3812e555",
   "metadata": {},
   "source": [
    "## How to Run Code\n",
    "\n",
    "Please include any special libraries and list your tf version here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad8329fa52fda5ea434e375b521aaa2477bf483cb1e2c0c679e9b1d37b903158"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
